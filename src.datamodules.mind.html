
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>src.datamodules.mind package &#8212; News Recommenders Library 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/insipid.css" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script defer src="_static/insipid.js"></script>
    <script defer src="_static/insipid-sidebar.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
  </head><body>
    <script type="text/javascript">
        document.body.classList.add('js');
    </script>
    <input type="checkbox" id="sidebar-checkbox" style="display: none;">
    <label id="overlay" for="sidebar-checkbox"></label>
    <div class="sidebar-resize-handle"></div>
    <script type="text/javascript">
        try {
            let sidebar = localStorage.getItem('sphinx-sidebar');
            const sidebar_width = localStorage.getItem('sphinx-sidebar-width');
            if (sidebar_width) {
                document.documentElement.style.setProperty('--sidebar-width', sidebar_width);
            }
            // show sidebar on wide screen
            if (!sidebar && window.matchMedia('(min-width: 100rem)').matches) {
                sidebar = 'visible';
                // NB: We don't store the value in localStorage!
            }
            if (sidebar === 'visible') {
                document.getElementById('sidebar-checkbox').checked = true;
            }
        } catch(e) {
            console.info(e);
        }
    </script>
    <header id="topbar-placeholder">
      <div id="topbar">
        <div id="titlebar">
          <div class="buttons">
            <label for="sidebar-checkbox" id="sidebar-button" role="button" tabindex="0" aria-controls="sphinxsidebar" accesskey="M">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg>
            </label>
            <button id="search-button" type="button" title="Search" aria-label="Search" aria-expanded="false" aria-controls="search-form" accesskey="S">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
            </button>
          </div>
          <div class="title">
            <a class="parent" href="index.html" accesskey="U">News Recommenders Library 0.0.1 documentation</a>
            <a class="top" href="#">src.datamodules.mind package</a>
          </div>
          <div class="buttons">
            <button id="fullscreen-button" type="button" aria-hidden="true">
              <span class="enable">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M212.686 315.314L120 408l32.922 31.029c15.12 15.12 4.412 40.971-16.97 40.971h-112C10.697 480 0 469.255 0 456V344c0-21.382 25.803-32.09 40.922-16.971L72 360l92.686-92.686c6.248-6.248 16.379-6.248 22.627 0l25.373 25.373c6.249 6.248 6.249 16.378 0 22.627zm22.628-118.628L328 104l-32.922-31.029C279.958 57.851 290.666 32 312.048 32h112C437.303 32 448 42.745 448 56v112c0 21.382-25.803 32.09-40.922 16.971L376 152l-92.686 92.686c-6.248 6.248-16.379 6.248-22.627 0l-25.373-25.373c-6.249-6.248-6.249-16.378 0-22.627z"/></svg>
              </span>
              <span class="disable">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M4.686 427.314L104 328l-32.922-31.029C55.958 281.851 66.666 256 88.048 256h112C213.303 256 224 266.745 224 280v112c0 21.382-25.803 32.09-40.922 16.971L152 376l-99.314 99.314c-6.248 6.248-16.379 6.248-22.627 0L4.686 449.941c-6.248-6.248-6.248-16.379 0-22.627zM443.314 84.686L344 184l32.922 31.029c15.12 15.12 4.412 40.971-16.97 40.971h-112C234.697 256 224 245.255 224 232V120c0-21.382 25.803-32.09 40.922-16.971L296 136l99.314-99.314c6.248-6.248 16.379-6.248 22.627 0l25.373 25.373c6.248 6.248 6.248 16.379 0 22.627z"/></svg>
              </span>
            </button>
          </div>
        </div>
        <div id="searchbox" role="search">
          <form id="search-form" class="search" style="display: none" action="search.html" method="get">
            <input type="search" name="q" placeholder="Search ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
            <button>Go</button>
          </form>
        </div>
      </div>
    </header>
    <nav>
    </nav>

    <nav class="relbar">
    </nav>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
<section id="src-datamodules-mind-package">
<h1>src.datamodules.mind package<a class="headerlink" href="#src-datamodules-mind-package" title="Permalink to this heading">§</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">§</a></h2>
</section>
<section id="module-src.datamodules.mind.datamodule_BERT">
<span id="src-datamodules-mind-datamodule-bert-module"></span><h2>src.datamodules.mind.datamodule_BERT module<a class="headerlink" href="#module-src.datamodules.mind.datamodule_BERT" title="Permalink to this heading">§</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.datamodule_BERT.</span></span><span class="sig-name descname"><span class="pre">MINDDataModuleBERT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mind_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'demo'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">column</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'title'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bert_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule" title="src.datamodules.mind.datamodule_Base.MINDDataModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">MINDDataModule</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.news_dataframe">
<span class="sig-name descname"><span class="pre">news_dataframe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.news_dataframe" title="Permalink to this definition">§</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data" title="Permalink to this definition">§</a></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single
process, so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In a distributed environment, <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways
(using <span class="xref std std-ref">prepare_data_per_node</span>)</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_dataloader</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup" title="Permalink to this definition">§</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong> – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.test_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.train_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">section</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id1"><span class="problematic" id="id2">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.val_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id3"><span class="problematic" id="id4">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup" title="src.datamodules.mind.datamodule_BERT.MINDDataModuleBERT.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.datamodules.mind.datamodule_Base">
<span id="src-datamodules-mind-datamodule-base-module"></span><h2>src.datamodules.mind.datamodule_Base module<a class="headerlink" href="#module-src.datamodules.mind.datamodule_Base" title="Permalink to this heading">§</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_Base.MINDDataModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.datamodule_Base.</span></span><span class="sig-name descname"><span class="pre">MINDDataModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mind_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_Base.MINDDataModule" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code></p>
<p>Base Datamodule for the MIND dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_dir</strong> (<em>str</em>) – Data directory</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size for dataloaders</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers for dataloaders</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to use pin memory</p></li>
<li><p><strong>download</strong> (<em>bool</em>) – Whether the mind dataset should be downloaded</p></li>
<li><p><strong>mind_size</strong> (<em>str</em>) – Which dataset size should be used</p></li>
<li><p><strong>train_val_test_split</strong> (<em>list</em>) – Whether to use automatic train-validation-test data splits</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_Base.MINDDataModule.prepare">
<span class="sig-name descname"><span class="pre">prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.prepare" title="Permalink to this definition">§</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_Base.MINDDataModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.setup" title="Permalink to this definition">§</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong> – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_Base.MINDDataModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.test_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.setup" title="src.datamodules.mind.datamodule_Base.MINDDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.setup" title="src.datamodules.mind.datamodule_Base.MINDDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_Base.MINDDataModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.train_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">section</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id5"><span class="problematic" id="id6">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.setup" title="src.datamodules.mind.datamodule_Base.MINDDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.setup" title="src.datamodules.mind.datamodule_Base.MINDDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_Base.MINDDataModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.val_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id7"><span class="problematic" id="id8">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule.setup" title="src.datamodules.mind.datamodule_Base.MINDDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.datamodules.mind.datamodule_CollaborativeFiltering">
<span id="src-datamodules-mind-datamodule-collaborativefiltering-module"></span><h2>src.datamodules.mind.datamodule_CollaborativeFiltering module<a class="headerlink" href="#module-src.datamodules.mind.datamodule_CollaborativeFiltering" title="Permalink to this heading">§</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.datamodule_CollaborativeFiltering.</span></span><span class="sig-name descname"><span class="pre">MINDDataModuleCollaborativeFiltering</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mind_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule" title="src.datamodules.mind.datamodule_Base.MINDDataModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">MINDDataModule</span></code></a></p>
<p>Datamodule for the Collaborative Filtering model using the MIND dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_dir</strong> (<em>str</em>) – Data directory</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size for dataloaders</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers for dataloaders</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to use pin memory</p></li>
<li><p><strong>download</strong> (<em>bool</em>) – Whether the mind dataset should be downloaded</p></li>
<li><p><strong>mind_size</strong> (<em>str</em>) – Dataset size</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare">
<span class="sig-name descname"><span class="pre">prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare" title="Permalink to this definition">§</a></dt>
<dd><p>Prepare data for model usage, prior to model instantiation.
Create ratings files for training, validation, testing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare_data" title="Permalink to this definition">§</a></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single
process, so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In a distributed environment, <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways
(using <span class="xref std std-ref">prepare_data_per_node</span>)</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_dataloader</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.setup" title="Permalink to this definition">§</a></dt>
<dd><p>Create ratings datasets, knowledge graph dataset for dataloaders.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.train_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">section</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id9"><span class="problematic" id="id10">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare_data" title="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.setup" title="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare_data" title="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.setup" title="src.datamodules.mind.datamodule_CollaborativeFiltering.MINDDataModuleCollaborativeFiltering.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.datamodules.mind.datamodule_MKR">
<span id="src-datamodules-mind-datamodule-mkr-module"></span><h2>src.datamodules.mind.datamodule_MKR module<a class="headerlink" href="#module-src.datamodules.mind.datamodule_MKR" title="Permalink to this heading">§</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.datamodule_MKR.</span></span><span class="sig-name descname"><span class="pre">MINDDataModuleMKR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_categories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_subcategories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_title_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_abstract_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_title_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_wikidata</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mind_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule" title="src.datamodules.mind.datamodule_Base.MINDDataModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">MINDDataModule</span></code></a></p>
<p>Datamodule for the MKR model using the MIND dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_categories</strong> (<em>bool</em>) – Whether the data preprocessing includes news categories</p></li>
<li><p><strong>use_subcategories</strong> (<em>bool</em>) – Whether the data preprocessing includes news subcategories</p></li>
<li><p><strong>use_title_entities</strong> (<em>bool</em>) – Whether the data preprocessing includes news title entities</p></li>
<li><p><strong>use_abstract_entities</strong> (<em>bool</em>) – Whether the data preprocessing includes news abstract entities</p></li>
<li><p><strong>use_title_tokens</strong> (<em>bool</em>) – Whether the data preprocessing includes news title tokens</p></li>
<li><p><strong>use_wikidata</strong> (<em>bool</em>) – Whether the data preprocessing includes additional news entity wikidata knowledge graph</p></li>
<li><p><strong>data_dir</strong> (<em>str</em>) – Data directory</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size for dataloaders</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers for dataloaders</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to use pin memory</p></li>
<li><p><strong>download</strong> (<em>bool</em>) – Whether the mind dataset should be downloaded</p></li>
<li><p><strong>mind_size</strong> (<em>str</em>) – Dataset size</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare">
<span class="sig-name descname"><span class="pre">prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare" title="Permalink to this definition">§</a></dt>
<dd><p>Prepare data for model usage, prior to model instantiation.
Download wikidata knowledge graph, create knowledge graph and ratings files for training, validation, testing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare_data" title="Permalink to this definition">§</a></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single
process, so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In a distributed environment, <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways
(using <span class="xref std std-ref">prepare_data_per_node</span>)</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_dataloader</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.setup" title="Permalink to this definition">§</a></dt>
<dd><p>Create ratings datasets and knowledge graph dataset for dataloaders.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.train_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">section</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id11"><span class="problematic" id="id12">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare_data" title="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.setup" title="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare_data" title="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.setup" title="src.datamodules.mind.datamodule_MKR.MINDDataModuleMKR.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.datamodules.mind.datamodule_NAML">
<span id="src-datamodules-mind-datamodule-naml-module"></span><h2>src.datamodules.mind.datamodule_NAML module<a class="headerlink" href="#module-src.datamodules.mind.datamodule_NAML" title="Permalink to this heading">§</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.datamodule_NAML.</span></span><span class="sig-name descname"><span class="pre">MINDDataModuleNAML</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_attributes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mind_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'small'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_clicked_news_a_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_abstract</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_freq_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_freq_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_confidence_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">negative_sampling_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">300</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">glove_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule" title="src.datamodules.mind.datamodule_Base.MINDDataModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">MINDDataModule</span></code></a></p>
<p>Datamodule for the NAML model using the MIND dataset</p>
<p>Code based on <a class="reference external" href="https://github.com/Microsoft/Recommenders">https://github.com/Microsoft/Recommenders</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_attributes</strong> (<em>dict</em>) – Attributes are set based on the model</p></li>
<li><p><strong>mind_size</strong> (<em>string</em>) – Size of the MIND Dataset (demo, small, large)</p></li>
<li><p><strong>data_dir</strong> (<em>Optional</em><em>[</em><em>string</em><em>]</em>) – Path of the data directory for the dataset</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size for dataloaders</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers for dataloaders</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Requires more memory but might imporve performance</p></li>
<li><p><strong>num_clicked_news_a_user</strong> (<em>int</em>) – Number of clicked news for each user</p></li>
<li><p><strong>num_words_title</strong> (<em>int</em>) – Number of words in the title</p></li>
<li><p><strong>num_words_abstract</strong> (<em>int</em>) – Number of words in the abstract</p></li>
<li><p><strong>word_freq_threshold</strong> (<em>int</em>) – Frequency threshold of words</p></li>
<li><p><strong>entity_freq_threshold</strong> (<em>int</em>) – Frequency threshold of entities</p></li>
<li><p><strong>entity_confidence_threshold</strong> (<em>float</em>) – Confidence threshold of entities</p></li>
<li><p><strong>negative_sampling_ratio</strong> (<em>int</em>) – Negative sampling ratio</p></li>
<li><p><strong>word_embedding_dim</strong> (<em>int</em>) – Dimension of word embeddings</p></li>
<li><p><strong>entity_embedding_dim</strong> (<em>int</em>) – Dimension of entity embeddings</p></li>
<li><p><strong>download</strong> (<em>bool</em>) – Enable the download and extraction of the MIND dataset. When set to false, extract data must be available in data_dir.</p></li>
<li><p><strong>glove_size</strong> (<em>int</em>) – Size of Glove embeddings to download</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.news_dataloader">
<span class="sig-name descname"><span class="pre">news_dataloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.news_dataloader" title="Permalink to this definition">§</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.prepare">
<span class="sig-name descname"><span class="pre">prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.prepare" title="Permalink to this definition">§</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup" title="Permalink to this definition">§</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong> – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.test_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup" title="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup" title="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.user_dataloader">
<span class="sig-name descname"><span class="pre">user_dataloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.user_dataloader" title="Permalink to this definition">§</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.val_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id13"><span class="problematic" id="id14">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup" title="src.datamodules.mind.datamodule_NAML.MINDDataModuleNAML.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.datamodules.mind.datamodule_RippleNet">
<span id="src-datamodules-mind-datamodule-ripplenet-module"></span><h2>src.datamodules.mind.datamodule_RippleNet module<a class="headerlink" href="#module-src.datamodules.mind.datamodule_RippleNet" title="Permalink to this heading">§</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.datamodule_RippleNet.</span></span><span class="sig-name descname"><span class="pre">MINDDataModuleRippleNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_categories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_subcategories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_title_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_abstract_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_title_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_wikidata</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mind_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <a class="reference internal" href="#src.datamodules.mind.datamodule_Base.MINDDataModule" title="src.datamodules.mind.datamodule_Base.MINDDataModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">MINDDataModule</span></code></a></p>
<p>Datamodule for the RippleNet model using the MIND dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_categories</strong> (<em>bool</em>) – Whether the data preprocessing includes news categories</p></li>
<li><p><strong>use_subcategories</strong> (<em>bool</em>) – Whether the data preprocessing includes news subcategories</p></li>
<li><p><strong>use_title_entities</strong> (<em>bool</em>) – Whether the data preprocessing includes news title entities</p></li>
<li><p><strong>use_abstract_entities</strong> (<em>bool</em>) – Whether the data preprocessing includes news abstract entities</p></li>
<li><p><strong>use_title_tokens</strong> (<em>bool</em>) – Whether the data preprocessing includes news title tokens</p></li>
<li><p><strong>use_wikidata</strong> (<em>bool</em>) – Whether the data preprocessing includes additional news entity wikidata knowledge graph</p></li>
<li><p><strong>data_dir</strong> (<em>str</em>) – Data directory</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size for dataloaders</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers for dataloaders</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to use pin memory</p></li>
<li><p><strong>download</strong> (<em>bool</em>) – Whether the mind dataset should be downloaded</p></li>
<li><p><strong>mind_size</strong> (<em>str</em>) – Dataset size</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare">
<span class="sig-name descname"><span class="pre">prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare" title="Permalink to this definition">§</a></dt>
<dd><p>Prepare data for model usage, prior to model instantiation
Download wikidata knowledge graph, create knowledge graph and ratings files for training, validation, testing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare_data" title="Permalink to this definition">§</a></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single
process, so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In a distributed environment, <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways
(using <span class="xref std std-ref">prepare_data_per_node</span>)</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_dataloader</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.setup" title="Permalink to this definition">§</a></dt>
<dd><p>Create ratings datasets for dataloaders.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.train_dataloader" title="Permalink to this definition">§</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">section</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id15"><span class="problematic" id="id16">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare_data" title="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.setup" title="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare_data" title="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.setup" title="src.datamodules.mind.datamodule_RippleNet.MINDDataModuleRippleNet.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.datamodules.mind.dataset">
<span id="src-datamodules-mind-dataset-module"></span><h2>src.datamodules.mind.dataset module<a class="headerlink" href="#module-src.datamodules.mind.dataset" title="Permalink to this heading">§</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.BaseDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">BaseDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">behaviors_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">news_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_attributes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_title</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_abstract</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_clicked_news_a_user</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.BaseDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>Base Dataset for training</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>behaviors_path</strong> (<em>str</em>) – Path to behaviors file</p></li>
<li><p><strong>news_path</strong> (<em>str</em>) – Path to news file</p></li>
<li><p><strong>dataset_attributes</strong> (<em>list</em>) – Dataset attributes</p></li>
<li><p><strong>(</strong><strong>)</strong> (<em>num_clicked_news_a_user</em>) – Number of title words</p></li>
<li><p><strong>(</strong><strong>)</strong> – Number of abstract words</p></li>
<li><p><strong>(</strong><strong>)</strong> – Number of clicked news</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.BehaviorsBERTDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">BehaviorsBERTDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">behaviors_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.BehaviorsBERTDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>Behaviors dataset for BERT model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>behaviors_path</strong> (<em>str</em>) – Path to behaviors file</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.BehaviorsDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">BehaviorsDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">behaviors_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.BehaviorsDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>User behaviors dataset for evaluation. (user, time) pair as session</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>behaviors_path</strong> (<em>str</em>) – Path to behaviors file</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.KGDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">KGDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">numpy_data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.KGDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>News knowledge graph dataset for dataloaders</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>numpy_data</strong> (<em>numpy.ndarray</em>) – Knowledge graph numpy data</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.NewsBERTDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">NewsBERTDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">news_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.NewsBERTDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>News dataset for BERT model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>news_path</strong> (<em>str</em>) – Path to news file</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.NewsDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">NewsDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">news_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.NewsDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>News dataset for evaluation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>news_path</strong> (<em>str</em>) – Path to news file</p></li>
<li><p><strong>dataset_attributes</strong> (<em>list</em>) – Dataset attributes</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.NewsDataset.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.NewsDataset.to" title="Permalink to this definition">§</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.RatingsDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">RatingsDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">numpy_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.RatingsDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>User Ratings knowledge graph dataset for dataloaders</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>numpy_data</strong> (<em>numpy.ndarray</em>) – Ratings numpy data</p></li>
<li><p><strong>train</strong> (<em>bool</em>) – Whether the dataset contains training data</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.datamodules.mind.dataset.UserDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.dataset.</span></span><span class="sig-name descname"><span class="pre">UserDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">behaviors_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user2int_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_clicked_news_a_user</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.dataset.UserDataset" title="Permalink to this definition">§</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>Users dataset for evaluation. Duplicated rows will be dropped</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>behaviors_path</strong> (<em>str</em>) – Path to behaviors file</p></li>
<li><p><strong>user2int_path</strong> (<em>str</em>) – Path to user index file</p></li>
<li><p><strong>(</strong><strong>)</strong> (<em>num_clicked_news_a_user</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-src.datamodules.mind.download">
<span id="src-datamodules-mind-download-module"></span><h2>src.datamodules.mind.download module<a class="headerlink" href="#module-src.datamodules.mind.download" title="Permalink to this heading">§</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.download_and_extract_glove">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">download_and_extract_glove</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">zip_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dest_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">glove_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.download_and_extract_glove" title="Permalink to this definition">§</a></dt>
<dd><p>Download and extract the Glove embedding</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dest_path</strong> (<em>str</em>) – Destination directory path for the downloaded file</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>File path where Glove was extracted.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.download_and_extract_mind">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">download_and_extract_mind</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'small'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dest_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.download_and_extract_mind" title="Permalink to this definition">§</a></dt>
<dd><p>Download and extract the MIND dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>str</em>) – Dataset size</p></li>
<li><p><strong>dest_path</strong> (<em>str</em>) – Save path for the zip dataset</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple (train_path, valid_path, test_path) where train_path is the path to the train folder, valid_path is the path to the validation folder and test_path is the path to the test folder</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.download_and_extract_wikidata_kg">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">download_and_extract_wikidata_kg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dest_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_zip_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.download_and_extract_wikidata_kg" title="Permalink to this definition">§</a></dt>
<dd><p>Download and extract the wikidata knowledge graph for the MIND dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dest_path</strong> (<em>str</em>) – Path for saving the downloaded zip file</p></li>
<li><p><strong>clean_zip_file</strong> (<em>bool</em>) – Whether to delete the zip file after unzipping</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the unzipped wikidata knowledge graph folder</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.extract_mind">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">extract_mind</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_zip</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_zip</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_zip</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root_folder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_folder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_folder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'valid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_folder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'test'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_zip_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.extract_mind" title="Permalink to this definition">§</a></dt>
<dd><p>Extract MIND dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_zip</strong> (<em>str</em>) – Path to train zip file</p></li>
<li><p><strong>valid_zip</strong> (<em>str</em>) – Path to valid zip file</p></li>
<li><p><strong>train_folder</strong> (<em>str</em>) – Destination folder for train set</p></li>
<li><p><strong>valid_folder</strong> (<em>str</em>) – Destination folder for validation set</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple (path_train, path_valid) where path_train is the path to the training folder and path_valid is the path to the validation folder</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.generate_embeddings">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">generate_embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">news_words</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">news_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sentence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.generate_embeddings" title="Permalink to this definition">§</a></dt>
<dd><p>Generate embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_path</strong> (<em>str</em>) – Data path.</p></li>
<li><p><strong>news_words</strong> (<em>dict</em>) – News word dictionary.</p></li>
<li><p><strong>news_entities</strong> (<em>dict</em>) – News entity dictionary.</p></li>
<li><p><strong>train_entities</strong> (<em>str</em>) – Train entity file.</p></li>
<li><p><strong>valid_entities</strong> (<em>str</em>) – Validation entity file.</p></li>
<li><p><strong>max_sentence</strong> (<em>int</em>) – Max sentence size.</p></li>
<li><p><strong>word_embedding_dim</strong> (<em>int</em>) – Word embedding dimension.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple containing the paths to the news, word and entity embeddings</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.get_train_input">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">get_train_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">session</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">npratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.get_train_input" title="Permalink to this definition">§</a></dt>
<dd><p>Generate train file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>session</strong> (<em>list</em>) – List of user session with user_id, clicks, positive and negative interactions.</p></li>
<li><p><strong>train_file_path</strong> (<em>str</em>) – Path to file.</p></li>
<li><p><strong>npratio</strong> (<em>int</em>) – Ratio for negative sampling.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.get_user_history">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">get_user_history</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_history</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_history</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_history_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.get_user_history" title="Permalink to this definition">§</a></dt>
<dd><p>Generate user history file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_history</strong> (<em>list</em>) – Train history.</p></li>
<li><p><strong>valid_history</strong> (<em>list</em>) – Validation history</p></li>
<li><p><strong>user_history_path</strong> (<em>str</em>) – Path to file.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.get_valid_input">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">get_valid_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">session</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_file_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.get_valid_input" title="Permalink to this definition">§</a></dt>
<dd><p>Generate validation file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>session</strong> (<em>list</em>) – List of user session with user_id, clicks, positive and negative interactions.</p></li>
<li><p><strong>valid_file_path</strong> (<em>str</em>) – Path to file.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.load_glove_matrix">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">load_glove_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_emb</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_embedding_dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.load_glove_matrix" title="Permalink to this definition">§</a></dt>
<dd><p>Load pretrained embedding metrics of words in word_dict</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path_emb</strong> (<em>string</em>) – Folder path of downloaded glove file</p></li>
<li><p><strong>word_dict</strong> (<em>dict</em>) – Word dictionary</p></li>
<li><p><strong>word_embedding_dim</strong> – Dimention of word embedding vectors</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Numpy.ndarray list containing pretrained word embedding metrics, words can be found in glove files</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.read_clickhistory">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">read_clickhistory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.read_clickhistory" title="Permalink to this definition">§</a></dt>
<dd><p>Read click history file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>str</em>) – Folder path</p></li>
<li><p><strong>filename</strong> (<em>str</em>) – Filename</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple (list, dict) where list is a list of user session with user_id, clicks, positive and negative interactions and dict is a dictionary with user_id click history.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.read_news">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">read_news</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filepath</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.read_news" title="Permalink to this definition">§</a></dt>
<dd><p>Read news file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filepath</strong> (<em>str</em>) – Path to news file</p></li>
<li><p><strong>tokenizer</strong> (<em>tokenizer</em>) – Tokenizer for news title tokenization</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple (news_words, news_entities, news_abstract_entities, news_categories, news_subcategories) where each item is a dictionary containing the items, specified by the dictionary name, assigned to each user</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.read_news_ids">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">read_news_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filepath</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.read_news_ids" title="Permalink to this definition">§</a></dt>
<dd><p>Read news ids</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>filepath</strong> (<em>str</em>) – Path to news file</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dictionary containing news identifiers and generated ids</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.download.word_tokenize">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.download.</span></span><span class="sig-name descname"><span class="pre">word_tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sent</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.download.word_tokenize" title="Permalink to this definition">§</a></dt>
<dd><p>Tokenize a sentence</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sent</strong> (<em>str</em>) – Sentence to be tokenized</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Word list</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-src.datamodules.mind.parse">
<span id="src-datamodules-mind-parse-module"></span><h2>src.datamodules.mind.parse module<a class="headerlink" href="#module-src.datamodules.mind.parse" title="Permalink to this heading">§</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.generate_word_embedding">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">generate_word_embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word2int_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_embedding_dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.generate_word_embedding" title="Permalink to this definition">§</a></dt>
<dd><p>Generate from pretrained word embedding file
If a word not in embedding file, initial its embedding by N(0, 1)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> (<em>str</em>) – Path of pretrained word embedding file, e.g. glove.840B.300d.txt</p></li>
<li><p><strong>target</strong> (<em>str</em>) – Path for saving word embedding</p></li>
<li><p><strong>word2int_path</strong> (<em>str</em>) – Path to vocabulary file</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.parse_behaviors">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">parse_behaviors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user2int_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">negative_sampling_ratio</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.parse_behaviors" title="Permalink to this definition">§</a></dt>
<dd><p>Parse behaviors file in training set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> (<em>str</em>) – Source behaviors file</p></li>
<li><p><strong>target</strong> (<em>str</em>) – Target behaviors file</p></li>
<li><p><strong>user2int_path</strong> (<em>str</em>) – Path for saving user2int file</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of users</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.parse_behaviors_bert">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">parse_behaviors_bert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">news_ids_set</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.parse_behaviors_bert" title="Permalink to this definition">§</a></dt>
<dd><p>Parse behaviors for using BERT baseline
Get all the history(NewsID) for each user
Get the candidate_news from impressions for each user
:param source: source news file
:param target: target news file</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>behaviors_parsed(id, history:&lt;NEWS_IDS&gt;, candidate_news&lt;NEWS_IDS&gt;, labels:&lt;y_true&gt;)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.parse_mind">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">parse_mind</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">glove_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">glove_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">negative_sampling_ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_title</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_abstract</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_confidence_threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_freq_threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_freq_threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_embedding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_embedding_dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.parse_mind" title="Permalink to this definition">§</a></dt>
<dd><p>Parse MIND dataset
:param train_dir: Path to train directory
:type train_dir: str
:param val_dir: Path to validation directory
:type val_dir: str
:param test_dir: Path to test directory
:type test_dir: str
:param glove_dir: Path to glove directory
:type glove_dir: str
:param glove_size (): Glove size
:param negative_sampling_ratio ():
:param num_words_title: number of words in title
:type num_words_title: long
:param num_words_abstract: number of words in abstract
:type num_words_abstract: long
:param entity_confidence_threshold ():
:param word_freq_threshold: Threshold for word frequency
:type word_freq_threshold: float
:param entity_freq_threshold: Threshold for entity frequency
:type entity_freq_threshold: float
:param word_embedding_dim (): Word embedding dimension
:param entity_embedding_dim (): Entity embedding dimension</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple (num_users, num_categories, num_words, num_entities) containing number of users, number of categories, number of words, number of entities</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.parse_mind_bert">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">parse_mind_bert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">column</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.parse_mind_bert" title="Permalink to this definition">§</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.parse_news">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">parse_news</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">category2int_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word2int_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity2int_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_title</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_words_abstract</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_confidence_threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_freq_threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_freq_threshold</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.parse_news" title="Permalink to this definition">§</a></dt>
<dd><p>Parse news for training set and test set</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> (<em>str</em>) – Source news file</p></li>
<li><p><strong>target</strong> (<em>str</em>) – Target news file</p></li>
<li><p><strong>category2int_path</strong> (<em>str</em>) – Path to category2int file. If mode == ‘train’: Path to save. If mode == ‘test’: Path to load from.</p></li>
<li><p><strong>word2int_path</strong> (<em>str</em>) – Path to word2int file. If mode == ‘train’: Path to save. If mode == ‘test’: Path to load from.</p></li>
<li><p><strong>entity2int_path</strong> (<em>str</em>) – Path to entity2int file. If mode == ‘train’: Path to save. If mode == ‘test’: Path to load from.</p></li>
<li><p><strong>mode</strong> (<em>str</em>) – Either ‘train’ or ‘test’</p></li>
<li><p><strong>num_words_title</strong> (<em>long</em>) – number of words in title</p></li>
<li><p><strong>num_words_abstract</strong> (<em>long</em>) – number of words in abstract</p></li>
<li><p><strong>(</strong><strong>)</strong> (<em>entity_confidence_threshold</em>) – </p></li>
<li><p><strong>word_freq_threshold</strong> (<em>float</em>) – Threshold for word frequency</p></li>
<li><p><strong>entity_freq_threshold</strong> (<em>float</em>) – Threshold for entity frequency</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.parse_news_bert">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">parse_news_bert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">column</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.parse_news_bert" title="Permalink to this definition">§</a></dt>
<dd><p>Parse news for using BERT baseline
Generate BERT embedding for the text in news df
:param source: source news file
:param target: target news file
:param column: the text that will represent the news</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>news_parsed(news_id, text)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.parse.transform_entity_embedding">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.parse.</span></span><span class="sig-name descname"><span class="pre">transform_entity_embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity2int_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entity_embedding_dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.parse.transform_entity_embedding" title="Permalink to this definition">§</a></dt>
<dd><p>Transform entity embedding</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> (<em>str</em>) – Path of embedding file</p></li>
<li><p><strong>target</strong> (<em>str</em>) – Path of transformed embedding file in numpy format</p></li>
<li><p><strong>entity2int_path</strong> (<em>str</em>) – Path to entity ids file</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-src.datamodules.mind.preprocessing">
<span id="src-datamodules-mind-preprocessing-module"></span><h2>src.datamodules.mind.preprocessing module<a class="headerlink" href="#module-src.datamodules.mind.preprocessing" title="Permalink to this heading">§</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.preprocessing.create_knowledge_graph_file">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.preprocessing.</span></span><span class="sig-name descname"><span class="pre">create_knowledge_graph_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">paths</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_categories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_subcategories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_title_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_abstract_entities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_title_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_wikidata</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.preprocessing.create_knowledge_graph_file" title="Permalink to this definition">§</a></dt>
<dd><p>Creates a news article knowledge graph file including the properties set in the constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>str</em>) – Model name, either “MKR” or “RippleNet”. Necessary for filepath.</p></li>
<li><p><strong>paths</strong> (<em>list</em>) – Contains paths to the train and/or validation and/or test news files.</p></li>
<li><p><strong>use_categories</strong> (<em>boolean</em>) – Whether to use news categories in knowledge graph.</p></li>
<li><p><strong>use_subcategories</strong> (<em>boolean</em>) – Whether to use news subcategories in knowledge graph.</p></li>
<li><p><strong>use_title_entities</strong> (<em>boolean</em>) – Whether to use news title entities in knowledge graph.</p></li>
<li><p><strong>use_abstract_entities</strong> (<em>boolean</em>) – Whether to use news abstract entities in knowledge graph.</p></li>
<li><p><strong>use_title_tokens</strong> (<em>boolean</em>) – Whether to use news title tokens in knowledge graph.</p></li>
<li><p><strong>use_wikidata</strong> (<em>boolean</em>) – Whether to use additional wikidata knowledge graph in knowledge graph.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple (path1, path2) where path1 is the path to the file containing the knowledge graph and path2 is the path to the file containing the item index to entity id hashes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.preprocessing.create_rating_file">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.preprocessing.</span></span><span class="sig-name descname"><span class="pre">create_rating_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paths</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_to_news_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.preprocessing.create_rating_file" title="Permalink to this definition">§</a></dt>
<dd><p>Creates a file specifying which news articles have been read by users and which have not</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>paths</strong> (<em>list</em>) – List containing paths to behaviours files (train, validation, test)</p></li>
<li><p><strong>path_to_news_ids</strong> (<em>str</em>) – Path to file containing news ids and corresponding item index</p></li>
<li><p><strong>model</strong> (<em>str</em>) – Model name, either “MKR” or “RippleNet”. Necessary for path</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.preprocessing.create_rating_file_collaborative">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.preprocessing.</span></span><span class="sig-name descname"><span class="pre">create_rating_file_collaborative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paths</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.preprocessing.create_rating_file_collaborative" title="Permalink to this definition">§</a></dt>
<dd><p>Creates a file specifying which news articles have been read by users and which have not. Exclusively for the Collaborative Filtering model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>paths</strong> (<em>list</em>) – List containing paths to behaviours files (train, validation, test)</p></li>
<li><p><strong>model</strong> (<em>str</em>) – Model name, either “MKR” or “RippleNet”. Necessary for path</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.preprocessing.prepare_numpy_data">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.preprocessing.</span></span><span class="sig-name descname"><span class="pre">prepare_numpy_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.preprocessing.prepare_numpy_data" title="Permalink to this definition">§</a></dt>
<dd><p>Converts the rating.txt file to numpy format</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>str</em>) – Path to the ratings.txt file</p></li>
<li><p><strong>model</strong> (<em>str</em>) – Model name, either “MKR” or “RippleNet”. Necessary for path</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Numpy.ndarray containing the rating data in numpy format</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.datamodules.mind.preprocessing.prepare_numpy_kg">
<span class="sig-prename descclassname"><span class="pre">src.datamodules.mind.preprocessing.</span></span><span class="sig-name descname"><span class="pre">prepare_numpy_kg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.datamodules.mind.preprocessing.prepare_numpy_kg" title="Permalink to this definition">§</a></dt>
<dd><p>Converts the knowledge graph.txt file to numpy format</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>str</em>) – Path to the knowledge graph.txt file</p></li>
<li><p><strong>model</strong> (<em>str</em>) – Model name, either “MKR” or “RippleNet”. Necessary for path</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>numpy.Ndarray containing the knowledge graph data in numpy format</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-src.datamodules.mind">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-src.datamodules.mind" title="Permalink to this heading">§</a></h2>
</section>
</section>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">

          <div class="sidebar-resize-handle"></div>

<h3><a href="index.html">Table of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models included in the library</a></li>
<li class="toctree-l1"><a class="reference internal" href="datamodules.html">Data modules for models included in the library</a></li>
</ul>

<hr class="docutils" />
<ul>
  <li class="toctree-l1"><a class="reference internal" href="genindex.html" accesskey="I">General Index</a></li>
  <li class="toctree-l1"><a class="reference internal" href="py-modindex.html">Python Module Index</a></li>
</ul>
<div id="ethical-ad-placement"></div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <nav class="relbar">
    </nav>

    <footer role="contentinfo">
      &#169; Copyright 2022, Noel Chia, Ahmed Elzamarany, Nadine Maeser.
      Created using <a class="reference external" href="https://www.sphinx-doc.org/">Sphinx</a> 5.2.3.
      <a class="reference external" href="https://insipid-sphinx-theme.readthedocs.io/">Insipid Theme</a>.
<a class="reference internal" href="_sources/src.datamodules.mind.rst.txt" rel="nofollow">Show Source</a>.
    </footer>
  </body>
</html>